syntax = "proto3";
package vllm_bootstrap;

service InferenceService {
  rpc Embed(EmbedRequest) returns (EmbedResponse);
  rpc Complete(CompleteRequest) returns (CompleteResponse);
}

message EmbedRequest {
  string launch_id = 1;
  repeated string texts = 2;
}
message EmbedResponse {
  repeated Embedding embeddings = 1;
}
message Embedding {
  repeated float values = 1;
}
message CompleteRequest {
  string launch_id = 1;
  string prompt = 2;
  int32 max_tokens = 3;
  optional float temperature = 4;
  optional float top_p = 5;
}
message CompleteResponse {
  string text = 1;
  int32 prompt_tokens = 2;
  int32 completion_tokens = 3;
}
