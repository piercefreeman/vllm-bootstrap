# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: inference.proto
# Protobuf Python Version: 6.31.1
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import runtime_version as _runtime_version
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
_runtime_version.ValidateProtobufRuntimeVersion(
    _runtime_version.Domain.PUBLIC,
    6,
    31,
    1,
    '',
    'inference.proto'
)
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0finference.proto\x12\x0evllm_bootstrap\"0\n\x0c\x45mbedRequest\x12\x11\n\tlaunch_id\x18\x01 \x01(\t\x12\r\n\x05texts\x18\x02 \x03(\t\">\n\rEmbedResponse\x12-\n\nembeddings\x18\x01 \x03(\x0b\x32\x19.vllm_bootstrap.Embedding\"\x1b\n\tEmbedding\x12\x0e\n\x06values\x18\x01 \x03(\x02\"i\n\x14GuidedDecodingParams\x12\x15\n\x0bjson_schema\x18\x01 \x01(\tH\x00\x12\x0f\n\x05regex\x18\x02 \x01(\tH\x00\x12\x11\n\x07grammar\x18\x03 \x01(\tH\x00\x12\x0e\n\x06\x63hoice\x18\x04 \x03(\tB\x06\n\x04kind\"L\n\nCompletion\x12\x0c\n\x04text\x18\x01 \x01(\t\x12\x15\n\rprompt_tokens\x18\x02 \x01(\x05\x12\x19\n\x11\x63ompletion_tokens\x18\x03 \x01(\x05\"\xe9\x01\n\x0f\x43ompleteRequest\x12\x11\n\tlaunch_id\x18\x01 \x01(\t\x12\x0f\n\x07prompts\x18\x02 \x03(\t\x12\x12\n\nmax_tokens\x18\x03 \x01(\x05\x12\x18\n\x0btemperature\x18\x04 \x01(\x02H\x00\x88\x01\x01\x12\x12\n\x05top_p\x18\x05 \x01(\x02H\x01\x88\x01\x01\x12\x42\n\x0fguided_decoding\x18\x06 \x01(\x0b\x32$.vllm_bootstrap.GuidedDecodingParamsH\x02\x88\x01\x01\x42\x0e\n\x0c_temperatureB\x08\n\x06_top_pB\x12\n\x10_guided_decoding\"C\n\x10\x43ompleteResponse\x12/\n\x0b\x63ompletions\x18\x01 \x03(\x0b\x32\x1a.vllm_bootstrap.Completion2\xc2\x02\n\x10InferenceService\x12\x44\n\x05\x45mbed\x12\x1c.vllm_bootstrap.EmbedRequest\x1a\x1d.vllm_bootstrap.EmbedResponse\x12M\n\x08\x43omplete\x12\x1f.vllm_bootstrap.CompleteRequest\x1a .vllm_bootstrap.CompleteResponse\x12H\n\x0b\x45mbedStream\x12\x1c.vllm_bootstrap.EmbedRequest\x1a\x19.vllm_bootstrap.Embedding0\x01\x12O\n\x0e\x43ompleteStream\x12\x1f.vllm_bootstrap.CompleteRequest\x1a\x1a.vllm_bootstrap.Completion0\x01\x62\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'inference_pb2', _globals)
if not _descriptor._USE_C_DESCRIPTORS:
  DESCRIPTOR._loaded_options = None
  _globals['_EMBEDREQUEST']._serialized_start=35
  _globals['_EMBEDREQUEST']._serialized_end=83
  _globals['_EMBEDRESPONSE']._serialized_start=85
  _globals['_EMBEDRESPONSE']._serialized_end=147
  _globals['_EMBEDDING']._serialized_start=149
  _globals['_EMBEDDING']._serialized_end=176
  _globals['_GUIDEDDECODINGPARAMS']._serialized_start=178
  _globals['_GUIDEDDECODINGPARAMS']._serialized_end=283
  _globals['_COMPLETION']._serialized_start=285
  _globals['_COMPLETION']._serialized_end=361
  _globals['_COMPLETEREQUEST']._serialized_start=364
  _globals['_COMPLETEREQUEST']._serialized_end=597
  _globals['_COMPLETERESPONSE']._serialized_start=599
  _globals['_COMPLETERESPONSE']._serialized_end=666
  _globals['_INFERENCESERVICE']._serialized_start=669
  _globals['_INFERENCESERVICE']._serialized_end=991
# @@protoc_insertion_point(module_scope)
